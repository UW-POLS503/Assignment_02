
## Correlated Variables

In the previous problem, the covariates were assumed to be independent.
Now, we will evaluate the properties of OLS estimates when covariates are correlated.
As before, the population is
$$
\begin{aligned}[t]
Y_i &= 0 + 1 \cdot x_{1,i} + 1 \cdot x_{2,i} + 1 \cdot x_{3,i} + \epsilon_i \\
\epsilon_i &\sim N(0, \sigma^2) \\
\sigma &= 1.7
\end{aligned}
$$
In this problem keep $\mu_X = (0, 0, 0)$ and $s_X = (1, 1, 1)$, but $R_X$ will differ between simulations to allow for different levels of correlation between $x_1$ and $x_2$.
The covariate $x_3$ is independent of the other covariates, $\cor(x_1, x_3) = \cor(x_2, x_3) = 0$.
Thus, the correlation matrix for $X$ in these simulations is the following, where $\rho_{1,2}$ will vary:
$$
R_X =
\begin{bmatrix}
1 & \rho_{1,2} & 0 \\
\rho_{1,2} & 1 & 0 \\
0 & 0 & 1 
\end{bmatrix}
$$

Simulate using `sim_lin_normal` with the following levels of correlation between $x_1$ and $x_2$ ($\rho_{1,2}$): 0, 0.5, 0.95, -0.5, -0.95
Based on the results of those simulations, how does $\cor(x_1, x_2)$ affect the following?

- The bias of each $\hat{\beta}_j$?
- The variance of each $\hat{\beta}_j$?
- The bias of the standard error of each $\hat{\beta}_j$?
- The bias of the robust standard error of each $\hat{\beta}_j$?

Remember to consider the effects of correlation on *all* the estimates: $\hat{\beta}_1$, $\hat{\beta}_2$, and $\hat{\beta}_3$.

What happens when $\rho = 1$ (or $\rho = -1$)? What assumption is violated?

## Omitted Variable Bias

The population is
$$
\begin{aligned}[t]
Y_i &= 0 + 1 \cdot x_{1,i} + 1 \cdot x_{2,i} + 1 \cdot x_{3,i} + \epsilon_i \\
\epsilon_i &\sim N(0, \sigma^2) \\
\sigma &= 1.7
\end{aligned}
$$

In all simulations, $(x_1, x_2)$ and $(x_2, x_3)$ are uncorrelated.
The correlation between $x_1$ and $x_3$ will vary between simulations.
In other words, the correlation matrix for the $x$ variables is
$$
R =
\begin{bmatrix}
1 & 0 & \rho_{1,3} \\
0 & 1 & 0 \\
\rho_{1,3} & 0 & 1 
\end{bmatrix}
$$

In all simulations, the sample regression will only include $x_1$ and $x_2$:
$$
y_i = \hat\beta_0 + \hat\beta_1 x_{1,i} + \hat\beta_2 x_{2,i} + \hat\epsilon_i
$$
Use $n = 1024$ for all simulations.

```{r}
sim_omitted_variables <- function(.data, beta, sigma, formula) {
  # X gives the number of observations in the data
  n <- nrow(X)
  # Draw data
  # This creates the X matrix
  X <- model.matrix(~., .data)
  # Create E(y | X)
  yhat <- X %*% beta
  # errors drawn from a normal distribution
  epsilon <- rnorm(n, mean = 0, sd = sigma)
  # actual y's
  y <- yhat + epsilon
  .data$y <- y
  # Estimate model
  mod <- lm(formula, data = .data)
  # Return results
  tidy(mod)
}
```

```{r, results='hide'}
n <- 1024
mu_X <- c(0, 0, 0)
s_X <- c(1, 1, 1)
rho <- 0
R_X <- matrix(c(1, 0, rho,
                0, 1, 0,
                rho, 0, 1), byrow = TRUE, nrow = 3)
X <- as.data.frame(rmvnorm(n, mu_X, sdcor2cov(s_X, R_X)))
beta <- c(0, 1, 1, 1)
sigma <- 1.7
#sim_omitted_variables(X, beta, sigma, x)
```

## Heteroskedasticity

Consider the case of bivariate regression with a single binary variable, in which each group has a different sample varaince:
$$
\begin{aligned}[t]
y_i &= \beta_0 + \beta_1 x_i + \epsilon_i \\
x_i &\in \{0, 1\} \\
\epsilon_i &\sim 
\begin{cases}
N(0, 1) & \text{if $x = 0$} \\
N(0, \sigma^2) & \text{if $x = 1$}
\end{cases}
\end{aligned}
$$

```{r}
sim_heteroskedasticity <- function(iter, x, beta, sigma) {
  mu <- cbind(1, x) %*% beta
  # variance varies by value of x
  sigma <- ifelse(as.logical(x), sigma, 1)
  epsilon <- rnorm(n, mean = 0, sd = sigma)
  # actual y's
  y <- yhat + epsilon
  # Estimate model
  mod <- lm(y ~ x)
  # Return results
  tidy(mod)
}
```


Estimate this with $\beta_0 = 0$, $\beta_1 = 1$, and varying values of sample size and $\sigma$? 

How do the following vary with $\sigma^2$ and $n$?

- bias and variance of `\beta_j`
- bias of `\se(\beta)_j`

## Non-random sample

This problem considers what happens when there is a truncated dependent variable.
This is also called sampling on the dependent variable, which is a research design problem not unknown to political science research.[^samplingdv]

The population is a linear normal model with homoskedastic errors.
$$
\begin{aligned}[t]
Y_1 &= \beta_0 + \beta_1 x_{1,i} + \dots + \beta_k x_{k,i} + \epsilon_i \\
\epsilon_i &\sim N(0, \sigma^2)
\end{aligned}
$$
However, in each sample, all $y_i$ which are less than a quantile $q$ are dropped before the regression is estimated.
$$
\begin{aligned}[t]
y_i = \beta_0 + \hat\beta_1 x_{1,i} + \dots + \hat\beta_k x_{k,i} + \hat\epsilon \\ \text{if $y_i \geq \quantile(y, q)$}
\end{aligned}
$$
where $\quantile(y, q)$ is the $q$th quantile of $y$.
For example, if $q = 0.5$, all $y_i$ that are less than the median of $y$ (the bottom 50%) are dropped.

The default value `truncation = 0.5` means all values of $y$ less than the median are dropped before running the regression.

Before running simulations, draw a single sample of a linear normal model with homoskedastic errors.
To do this, you should be able to adapt the code from `sim_lin_normal_truncated`.
Create a scatter plot with the OLS line for all $y$, and a another plot with only those $y$ less than the median of $y$.
How does the OLS line estimated on the truncated data differ from the one estimated on the full data.

Run several simulations with `sim_lin_normal_truncated` and vary the sample size.
How does the sample size affect the following:

- The bias of each $\hat{\beta}_j$?
- The variance of each $\hat{\beta}_j$?

In particular, if we gather more data but $y$ is truncated, does it decrease the bias in $\hat{\beta}$?

[^randomx]: Although the statistical theory of OLS works (thankfully) for random $X$,
    as long as certain conditions are met. See Fox (2nd ed.), Ch 9.6.

[^samplingdv]: See Ashworth, Scott, Joshua D. Clinton, Adam Meirowitz, and Kristopher W. Ramsay. 2008. ``Design, Inference, and the Strategic Logic of Suicide Terrorism.'' *American Political Science Review() 102(02): 269â€“73. <http://journals.cambridge.org/article_S0003055408080167>

```{r}
sim_truncated <- function(iter, .data, beta, sigma, q) {
  # X gives the number of observations in the data
  n <- nrow(X)
  # Draw data
  # This creates the X matrix
  X <- model.matrix(~., .data)
  # Create E(y | X)
  yhat <- X %*% beta
  # errors drawn from a normal distribution
  epsilon <- rnorm(n, mean = 0, sd = sigma)
  # actual y's
  y <- yhat + epsilon
  .data$y <- y
  # Remove all observations in which y is above the mean
  .data <- filter(.data, y > quantile(y, q))
  # Estimate model
  mod <- lm(y ~ ., data = .data)
  # Return results
  tidy(mod)
}
```


