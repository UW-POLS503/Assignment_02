---
title: "Assignment 02"
output:
  html_document:
    keep_md: true
---

1. [Fork this repository](https://help.github.com/articles/using-pull-requests/) to your GitHub account.
2. Write your solutions in R Markdown in a file named `solutions.Rmd`.
3. When you are ready to submit your assignment, [initiate a pull request](https://help.github.com/articles/using-pull-requests/#initiating-the-pull-request). Title your
pull request "Submission".

To update your fork from the upstream repository:

1. On your fork, e.g. `https://github.com/jrnold/Assignment_02` click on "New Pull reqest"
2. Set your fork `jrnold/Assignment_02` as the base fork on the left, and `UW-POLS503/Assignment_02` as the head fork on the right. In both cases the branch will be master. This means, compare any chanes in the head fork that are not in the base fork. You will see differences between the `US-POLS503` repo and your fork. Click on "Create Pull Request", and if there are no issues, "Click Merge" A quick way is to use this link, but change the `jrnold` to your own username: `https://github.com/jrnold/Assignment_02/compare/master...UW-POLS503:master`.


# Libraries used 

```{r message=FALSE}
library("pols503")
library("mvtnorm")
library("ggplot2")
library("dplyr")
library("broom")
```
If you do not have the **pols503** package installed, you can install it with,
```{r eval=FALSE}
library("devtools")
install_github("UW-POLS503/r-pols503")
```

# OLS Estimator

The purpose of this homework is to provide a guided, hands-on tour through the properties of the least squares estimator, especially under common violations of the Gauss Markov assumptions. We will work through a series of programs which use simulated data --- i.e., data created with known properties --- to investigate how these violations affect the accuracy and precision of least squares estimates of slope parameters. Using repeated study of simulated datasets to explore the properties of statistical models is called Monte Carlo experimentation. Although you will not have to write much R code, you will need to read through the provided programs carefully to understand what is happening.

Monte Carlo experiments always produce the same results as analytic proofs for the specific case considered. Each method has advantages and disadvantages: proofs are more general and elegant, but are not always possible. Monte Carlo experiments are much easier to construct and can always be carried out, but findings from these experiments only apply to the specific scenario under study. Where proofs are available, they are generally preferable to Monte Carlo experiments, but proofs of the properties of more complicated models are sometimes impossible or impractically difficult. This is almost always the case for the properties of models applied to small samples of data. Here, we use Monte Carlo not out of necessity but for pedagogical purposes, as a tool to gain a more intuitive and hands-on understanding of least squares and its properties.

All of the simulations in this assignment will follow the same structure:

1. Define a population
2. Repeat $m$ times:

    1. Draw a sample from the population
    2. Run OLS on that sample
    3. Save statistics, e.g. coefficients, standard errors, $p$-values, from the sample regression.

3. Evaluate the distributions of the sample statistics, or summaries thereof, to determine how well OLS recovers the parameters of the population.

In this section, we will work through the code necessary to run a simulation.
However, in the problems, functions written for this problem set will do most of the simulation computation. 
This section is to help you to understand what those functions are doing, and to provide a mapping from the math to the code.


## Sampling Distribution of OLS

Start with example in which the population satisfies all the Gauss-Markov assumptions and we run a correctly specified regression on the samples drawn from that population.

In this example, the population model is
$$
\begin{aligned}[t]
Y_i &= \beta_0 + \sum_{j = 1}^k \beta_j x_{i,j} + \epsilon_i \\
\epsilon_i & \sim N(0, \sigma^2)
\end{aligned}
$$
For a sample $y$ from that population, the OLS regression which will be run is
$$
\begin{aligned}[t]
y_i &= \hat\beta_0 + \sum_{j = 1}^k \hat\beta_j x_{i,j} + \hat\varepsilon_i
\end{aligned}
$$
and the estimate of the population variance is
$$
\widehat\sigma^2 &= \frac{\sum \hat\varepsilon_i }{n - k - 1} .
$$
In this case, the regression run on the samples has the correct specification, but that will not necessarily be true for other exercises.

Let's review the steps that we need to take for this problem.

1. Draw data from the population
2. Estimate the OLS parameters on that data
3. Return data 
4. Repeat 1-3 many times

Of these steps, the meat of the problem is in steps 1--3. Once we have that done, we can 
repeat it many times with a `for` loop.

The specified population here is
$$
Y_i &= \beta_0 + \sum_{j = 1}^k \beta_j x_{i,j} + \varepsilon_i \\
\varepsilon_i & \sim N(0, \sigma^2)
$$
What is known (or we need to specify) are the regression parameters, $\beta_0, \dots, \beta_j$,
the population variance, $\sigma^2$, and the predictors, $\mat{X}$.

Let's choose to simulate from the population:
$$
Y_i &= 0 + 1 x_{i,j} + \varepsilon_i
$$
This means that $\vec{\beta} = (0, 1)'$
```{r}
beta <- c(0, 1)
```
We also need to choose a sample size, $n$, and the values of $\mat{X}$.
While OLS in no way requires $\mat{X}$ to be distributed normal, for these simulations it 
will be convenient to simulate $\mat{X}$ from a normal distribution, since we can easily choose
the mean and variance of the sample.
Let's specify that that the sample size is $n = 30$, and the mean and standard deviations are $\bar{\vec{x}} = 0$ and $\bar{\vec{x}} = 1$.
```{r}
n <- 30
x_mean <- 0
x_sd <- 1
```
Now let's draw a sample of size `r n` from a standard normal distribution, $N(0, 1)$, using the function `rnorm`,
```{r}
x_raw <- rnorm(n, mean = 0, sd = 1)
```
Note that even though the sample was drawn from a normal distribution with the mean and standard deviation 
that we wanted, due to sampling variation, the sample mean and standard deviation will not be exactly 0 and 1, respectively.
```{r}
mean(x_raw)
sd(x_raw)
```
So, substract the sample mean and divide by the sample standard deviation to get a sample with a mean of 0 and standard deviation of 1,

```{r}
x <- (x_raw - mean(x_raw)) / sd(x_raw)
mean(x)
sd(x)
```
The R function [scale](http://www.rdocumentation.org/packages/base/functions/scale) will also do this.

Now, create a data frame with the predictors,
```{r}
dataset <- data.frame(X1 = x)
```
For the computations, we will need to use the design matrix of the predictors.
The R function `model.matrix` converts a formula to a design matrix.
```{r}
X <- model.matrix(~ X1, data = dataset)
head(X)
```
Note, that unlike `dataset`, `X` has a column of 1's for the intercept.
The function `model.matrix` also converts categorical variables into dummy variables, expand interactions, and evaluates functions in the formula.

To calculate the outcome variable, we still need to choose a variance from which the errors are drawn, since $\epsilon_i \sim N(0, \sigma^2)$.
I'll choose a value of $\sigma$ that will give the regression an $R^2$ of 0.5.
The function `pols503::r2_to_sigma` can calculate this.
```{r}
sigma <- r2_to_sigma(X, beta, 0.5)
sigma
```

Now, we need to calculate the outcome variable, $y$,
$$
\vec{y} = \mat{X} \vec{\beta} + \vec{\varepsilon} = \vec{\hat{y}} + \vec{\varepsilon} .
$$
First, calculate $\hat{\vec{y}}$:
```{r}
y_hat <- X %*% beta
```
Next, sample $\varepsilon_i \sim \dnorm(0, \sigma^2)$:
```{r}
eps <- rnorm(n, mean = 0, sd = sigma)
```
Finally, calculate $\vec{y}$ from the sum of the expected values and errors,
```{r}
y <- y_hat + eps
```
Append this to the dataset we are using,
```{r}
dataset[["y"]] <- y
```

Now that we have drawn a sample $\vec{y}$ from the population, we want to estimate $\beta$ using OLS,
```{r}
mod <- lm(y ~ X1, data = dataset)
mod
```
For ease of analysis later, we convert the coefficient results to a data frame using the **broom** function [tidy](http://www.rdocumentation.org/packages/broom/functions/tidy).
```{r}
tidy(mod)
```

The problem is that we want to do this multiple times.
Within a simulation, we will need to run multiple iterations.
And in different iterations, we will draw simulations with different samples.

First, create a function to sample $n$ observations of $X$ from a multivariate normal distribution for given means, standard deviations, and
correlation between the variables.
```{r}
normal_X <- function(n, mean, sd, cor) {
  X <- as.data.frame(MASS::mvrnorm(n, mu = 0, Sigma = sdcor2cov(sd, cor)))
  names(X) <- paste0("x", seq_len(ncol(X)))
  X                     
}
```

Second, create a function to sample from the population.
```{r}
sample_linear_normal <- function(Xdata, beta, sigma) {
  X <- model.matrix(~ ., data = Xdata)
  y_hat <- X %*% beta
  eps <- rnorm(n, mean = 0, sd = sigma)
  y <- y_hat + eps
  Xdata[["y"]] <- y
  Xdata
}
```

Now we will repeat this process multiple times.
First, create an empty `list` object to store the data frames returned
by `sim_linear_normal` in each iteration.
```{r}
results <- list()
```
Because this may take some time, let's create a progress bar using the
function [progress_estimated](http://www.rdocumentation.org/packages/dplyr/functions/progress_estimated) from the **dplyr** package.
```{r}
p <- progress_estimated(iter, min_time = 2)
```
Now run `sim_linear_normal()` `r iter` times with a `for` loop.[^for]
In each iteraction, we also add a variable to the results indicating the iteration, and update the progress bar.
```{r}
for (i in seq_len(.iter)) {
    .data <- sim_linear_normal(.data, beta, sigma) %>%
      mutate(.iter = i)
    results[[i]] <- .data
    p$tick()$print()
  }
```
Now that the iterations are completed, `results` is a list in which each element is a data frame. But for analysis it will be easier to work with a single data frame.
So, we stack the data frames on top of each other to create a single data frame,
```{r}
results <- bind_rows(results)
head(results)
```

[^for]: One downside of using a `for` loop is that things are run sequentially, even though the results of one iteration do not depend on any other iteration. Taking advangage of modern hardware, we could make this faster by rewriting the for-loop above to run in parallel using [mclapply](http://www.rdocumentation.org/packages/parallel/functions/mclapply.html) from the built-in **parallel** package, or [foreach](http://www.rdocumentation.org/packages/foreach/functions/foreach) from the **foreach** package, among others.

Now that we have the results in a data frame, to make sense of them, we need to summarize them.
There are two estimators we are interested in: $\hat{\beta}$ and $\hat{\se}{\beta}$.
So get the mean and standard deviation of the estimates, $\hat{\beta}$, and the standard errors, $\hat{\se}{\beta}$.
```{r}
results %>%
  group_by(term) %>%
  summarize(estimate_mean = mean(estimate),
            estimate_sd = sd(estimate),
            std_error_mean = mean(std.error),
            std_error_sd = sd(std.error))
```


Monte Carlo standard error is the error in simulations due to using only a finite number of samples. In other words, Monte Carlo is the sampling error in simulations.
In order to ensure that we have enough simulations to get accurate estimates of the parameters of the sampling distribuitons of $\hat{\beta}$ and $\hat{\se}{\beta}$, we should include variables with the Monte Carlo error of the simulation estimates.
The standard error of a mean is $\sigma / \sqrt{n}$, and the standard error of a standard deviation is approximately $\sigma / \sqrt{2 * (n - 1)}$.
So 



After these simulations we want to summarize the results of the coefficients.
```{r}
summarize_params <- function(.data) {
  ret <- .data %>%
    group_by(term) %>%
    summarize(estimate_mean = mean(estimate),
              estimate_sd = sd(estimate),
              std_error_mean = mean(std.error),
              std_error_sd = sd(std.error),
              estimate_mean_se = sd(estimate) / sqrt(n()),
              estimate_sd_se = sd(estimate) / sqrt(2 * (n() - 1)),
              std_error_mean_sd = sd(std.error) / sqrt(n()),
              std_error_sd_se = sd(std.error) / sqrt(2 * (n() - 1)),
              iter = length(estimate))
  ret
}
```

